---
title: "GAMs_final2"
format: html
editor: visual
---

Code to select the best model for prediction of blue crab predators through generalized additive models.

Helpful articles: -Setup initial GAM: https://bcheggeseth.github.io/253_fall_2021/local-regression-gams.html -Best for CV: https://www.tidymodels.org/start/resampling/ -An Introduction to Calibration wiht tidymodels: https://www.tidymodels.org/learn/models/calibration/

```{r}
source("~/Documents/GitHub/NCBlueCrab_Predators/Scripts/Load.SDM.Final.R")

#Divide datasets
div_ss <- floor(0.7*nrow(df_CPUE_length_wide_both))
picked= sample(seq_len(nrow(df_CPUE_length_wide_both)),size = div_ss)

#CV dataset
df_CV = df_CPUE_length_wide_both[picked,]
```

```{r}
library(gtools)

pastePerm<- function(row, names){
  keep<- which(row==1)
  if(length(keep)==0){
    return('1')
  }else{
    return(paste(names[keep],collapse='+'))
  }
}
my_sqrt <- function(var1){
  sqrt(var1) #take square root of variable 
} #construct model formulas 

dredgeform<- function(pred, covars, alwaysIn='factor(Yearfactor)'){ #always in is set to factor Year
  p<- length(covars) #number of independent variables
  perm.tab<- permutations(2, p, v=c(0,1), repeats.allowed=T) #for different combinations of predictor variables
  myforms<- NULL #store formulas 
  for(j in 1:nrow(perm.tab)){
    myforms[j]<- pastePerm(perm.tab[j,], c(alwaysIn, covars)) #function above
  }
  myforms<- paste0(pred, '~', alwaysIn, '+', myforms) #predicted variable and formula
  return(myforms)
}
```

```{r}
allformulas_reddrum <- dredgeform(pred = "reddrumP915", covars = c("s(avgdepth)", "s(avgssal)", "s(avgstemp)", "s(avgsdo)", "s(SAVkm)", "s(NoFishRest)"))

f <- "reddrumP915~factor(Yearfactor)+1"  
rhs <- strsplit(f, "~")[[1]][2]
rhs_string <- paste(rhs, collapse = ",") #this isn't working 
terms <- sub(".* ~ (.+?)(?:\\+1)?$", "\\1", f)
  terms <- gsub("\\+1", "", terms)
  terms <- gsub(" ", ", ", terms)
```

###Cross-validation

Using k-fold cross-validation, we build a gam() model on 4/5ths of the data, leaving 1/5 out. We do this a bunch of different times for each variable combination and get out R2 or RMSE.

K-fold cross-validation is used to find the best model for prediction. The data is randomly split into k-subsets (k-fold) let's say 5. 4/5 of the data is for training the model and 1/5 of the data is for testing the model. Each fold is used as a validation set once where the remaining folds forms the training set. The prediction error is calculated. This process continues until each set of data has functioned as the test set.

Typically, a k value of 5 or 10 is selected. The lower the k, the more biased the error rate estimates are.

With repeated k-fold cross-validation, the data is split into different folds. If there are 3 repeats on 5-fold cross-validation, the data is split into 5 different folds 3 times. The cross-validation procedure is repeated multiple times.

This is all done based on the initial data split into testing vs. training dataset. The training dataset is typically divided up once and this is used in model building.

```{r}
#Split the data once 
set.seed(123) #only divide data once 
#4/5, 1/5ths 
data_split <- initial_split(df_CV, strata = reddrumP915, prop= 0.80)

#Return split datasets 
data_train <- training(data_split) 
data_test  <- testing(data_split)

#rsample::initial_split takes original data and saves information on how to make the partitions 
#strata: conducts a stratified split, a variable in data used to conduct stratified sampling, random sampling is conducted within the stratification variable (resamples have equivalent proportions as the original data set), strata is binned into quartiles which are then used to stratify
#training dataset is choosen for resampling 
```

```{r}
#Set of 5-fold cross-validation indices for model re-sampling
folds <- vfold_cv(data_train, v= 5, strata= reddrumP915, repeats= 1) #10 fold cross-validation repeated 5 times on the training set 
```

```{r}
#Define model 
gam_mod <- gen_additive_mod() %>%
set_engine(engine = 'mgcv', family= tw(link= "log")) %>% set_mode('regression')

#set_engine: specifies package and any arguments specific to that software 
#outcome variable as continuous
```

```{r}
#Fit model
set.seed(123)
gam_wf <- workflow() %>% add_variables(outcomes= reddrumP915, predictors= c(avgstemp, avgssal, Yearfactor, avgsdo)) %>% add_model(gam_mod, formula= reddrumP915 ~ s(avgstemp) + s(avgssal) + Yearfactor + s(avgsdo)) %>% fit(data= data_train) 

#Perform cross-validation
gam_fit_rs <- gam_wf %>% fit_resamples(folds) 
t <- collect_metrics(gam_fit_rs)
reddrum_env <- as.data.frame(matrix(ncol = 3, nrow = 0))
colnames(reddrum_env) <- c("formula", "rmse", "rsq")
reddrum_env[i, 1] <- allformulas_reddrum[i]
reddrum_env[i, 2] <- t$mean[1]
reddrum_env[i, 3] <- t$mean[2]

#workflow: creates workflow, aggregate information required to fit and predict from a model
#fit_resamples(): analysis set (division within training data set) is used for prediction, 4/5 of data are used to fit the model, 1/5 is used for predictions, 5 predictions in non-repeated CV 
```

```{r}
compare_var_tw_reddrum <- as.data.frame(matrix(ncol = 2, nrow = 0))
colnames(compare_var_tw_reddrum) <- c("formula", "AIC")
for (i in 1:length(allformulas_reddrum)) {
model_tw_reddrum <- gam(as.formula(allformulas_reddrum[i]), data=df_inf_train, family= tw(link= "log"))
compare_var_tw_reddrum[i, 1] <- allformulas_reddrum[i]
compare_var_tw_reddrum[i, 2] <- AIC(model_tw_reddrum)
print(i)
}
```

```{r}
#Interpret
fit_gam %>% pluck('fit') %>% summary() 
par(mfrow=c(2,2))
fit_gam %>% pluck('fit') %>% mgcv::gam.check() 
fit_gam %>% pluck('fit') %>% plot( all.terms = TRUE, pages = 1)

gam_training_pred <- predict(fit_gam, data_test) %>% bind_cols(data_test %>% dplyr::select(reddrumP915))
```
